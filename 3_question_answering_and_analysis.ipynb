{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b59ab217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HP\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import pipeline\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee807604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the small generative model for Q&A\n",
    "flan_t5_qa = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "021dbb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a small, fast sentence encoder\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5e10d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 4 articles from summary DB.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(\"summary_db.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    summary_db = json.load(f)\n",
    "\n",
    "print(f\"✅ Loaded {len(summary_db)} articles from summary DB.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5165008",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def answer_from_article_flan_t5(question, summary, full_text):\n",
    "    if not summary and not full_text:\n",
    "        return \"(No content available.)\", 0.0\n",
    "\n",
    "    context_parts = []\n",
    "    if summary:\n",
    "        context_parts.append(summary)\n",
    "    if full_text:\n",
    "        context_parts.append(full_text)\n",
    "\n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "\n",
    "    if len(context.split()) > 800:\n",
    "        context = \" \".join(context.split()[:800])\n",
    "\n",
    "    # ✅ Use better, instruct-style prompt\n",
    "    prompt = f\"\"\"You are an expert news analyst.\n",
    "    Based on the context below, provide a detailed and factual answer to the question.\n",
    "\n",
    "### Question:\n",
    "{question}\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### Answer:\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = flan_t5_qa(prompt,\n",
    "                    max_new_tokens=300,  # allow more tokens\n",
    "                    min_new_tokens=80,  # encourage longer output\n",
    "                    do_sample=False,\n",
    "                    early_stopping=False\n",
    "                    )[0][\"generated_text\"].strip()\n",
    "\n",
    "        return response, 1.0\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Flan-T5 QA error: {e}\")\n",
    "        return \"(Error while answering)\", 0.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63246667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_articles(question, summary_db, top_k=5):\n",
    "    article_texts = []\n",
    "    article_keys = []\n",
    "\n",
    "    for title, entry in summary_db.items():\n",
    "        summary = entry.get(\"summary\", \"\")\n",
    "        if summary:\n",
    "            article_keys.append(title)\n",
    "            article_texts.append(f\"{title}. {summary}\")\n",
    "    \n",
    "    # Embed question and articles\n",
    "    question_embedding = embedder.encode(question, convert_to_tensor=True)\n",
    "    article_embeddings = embedder.encode(article_texts, convert_to_tensor=True)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarities = util.pytorch_cos_sim(question_embedding, article_embeddings)[0]\n",
    "    top_results = similarities.topk(top_k)\n",
    "\n",
    "    # Get top titles\n",
    "    top_titles = [article_keys[i] for i in top_results[1]]\n",
    "    return top_titles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "161968f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# def extractive_fallback(question, summary, full_text):\n",
    "#     def best_match(sentence_pool):\n",
    "#         question_words = set(re.findall(r'\\b\\w+\\b', question.lower()))\n",
    "#         best_sent, max_hits = \"\", 0\n",
    "#         for sent in sentence_pool:\n",
    "#             sent_words = set(re.findall(r'\\b\\w+\\b', sent.lower()))\n",
    "#             hits = len(question_words & sent_words)\n",
    "#             if hits > max_hits:\n",
    "#                 max_hits, best_sent = hits, sent\n",
    "#         return best_sent if max_hits > 0 else None\n",
    "\n",
    "#     fallback_sources = []\n",
    "#     if summary:\n",
    "#         summary_sentences = re.split(r'(?<=[.!?])\\s+', summary.strip())\n",
    "#         summary_hit = best_match(summary_sentences)\n",
    "#         if summary_hit:\n",
    "#             fallback_sources.append(f\"Summary fallback: {summary_hit}\")\n",
    "\n",
    "#     if full_text:\n",
    "#         full_sentences = re.split(r'(?<=[.!?])\\s+', full_text.strip())\n",
    "#         text_hit = best_match(full_sentences)\n",
    "#         if text_hit:\n",
    "#             fallback_sources.append(f\"Full text fallback: {text_hit}\")\n",
    "\n",
    "#     return \"\\n\".join(fallback_sources) if fallback_sources else \"(Fallback found nothing meaningful.)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a357588e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question_over_db_flan_t5(question, summary_db, max_results=1):\n",
    "    top_titles = get_relevant_articles(question, summary_db, top_k=max_results)\n",
    "    top_answers = []\n",
    "\n",
    "    for title in top_titles:\n",
    "        entry = summary_db[title]\n",
    "        summary = entry.get(\"summary\", \"\")\n",
    "        full_text = entry.get(\"full_text\", \"\")\n",
    "        url = entry.get(\"url\", \"\")\n",
    "\n",
    "        answer, _ = answer_from_article_flan_t5(question, summary, full_text)\n",
    "\n",
    "        top_answers.append({\n",
    "            \"title\": title,\n",
    "            \"url\": url,\n",
    "            \"answer\": answer\n",
    "        })\n",
    "\n",
    "    return top_answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ff53b9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (932 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: This Experimental Tech Allows Surgeons to See Through Blood\n",
      "Answer: HemoLucence. It reportedly uses AI-powered physics to digitally visualize blood as though it were translucent, which should give surgeons a clear view of the tissue beneath while operating. The technology is part of a surgical microscope system that the company plans to test in clinical trials as early as this year. The technology is part of a surgical microscope system that the company plans to test in clinical trials as early as this year.\n",
      "URL: https://gizmodo.com/this-experimental-tech-allows-surgeons-to-see-through-blood-2000620162\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_question = \"What allows surgeons to see through blood?\"\n",
    "answers = answer_question_over_db_flan_t5(test_question, summary_db)\n",
    "for ans in answers:\n",
    "    print(f\"Title: {ans['title']}\\nAnswer: {ans['answer']}\\nURL: {ans['url']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8c8c09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
