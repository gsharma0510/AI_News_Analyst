{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45138c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import json\n",
    "from newspaper import Article\n",
    "import time\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "import trafilatura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8ff036f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HP\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load small generative model for summarization and Q&A\n",
    "flan_t5 = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5120a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 10 resolved articles.\n"
     ]
    }
   ],
   "source": [
    "# Load article URLs from the previous notebook\n",
    "with open(\"resolved_articles.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    resolved_articles = json.load(f)\n",
    "\n",
    "print(f\"âœ… Loaded {len(resolved_articles)} resolved articles.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38b7daf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block clearly invalid or redirecting domains\n",
    "def is_valid_news_domain(url):\n",
    "    bad_domains = [\n",
    "        \"cloudflare.com\", \"facebook.com\", \"linkedin.com\", \"instagram.com\",\n",
    "        \"forbes.com/sites\", \"subscribe.bloomberg.com\", \"t.co\", \"youtube.com\"\n",
    "    ]\n",
    "    parsed = urlparse(url)\n",
    "    full_domain = parsed.netloc + parsed.path\n",
    "    return not any(bad in full_domain for bad in bad_domains)\n",
    "\n",
    "# More refined relevance scoring\n",
    "def is_content_relevant(text, title, min_words=100, keyword_match_threshold=0.2):\n",
    "    if not text or len(text.split()) < min_words:\n",
    "        return False\n",
    "    if title:\n",
    "        title_words = set(re.findall(r'\\b\\w+\\b', title.lower()))\n",
    "        text_words = set(re.findall(r'\\b\\w+\\b', text.lower()))\n",
    "        common = title_words & text_words\n",
    "        score = len(common) / len(title_words) if title_words else 0\n",
    "        return score >= keyword_match_threshold\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d51617bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main extraction function\n",
    "def get_article_text(url, title_hint=None):\n",
    "    try:\n",
    "        if not is_valid_news_domain(url):\n",
    "            print(f\"ğŸš« Blocked domain: {url}\")\n",
    "            return \"Blocked Domain\", None\n",
    "\n",
    "        title = title_hint or \"Untitled Article\"\n",
    "        np_text = \"\"\n",
    "        tf_text = \"\"\n",
    "\n",
    "        # Try Newspaper3k\n",
    "        try:\n",
    "            article = Article(url)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            title = article.title or title_hint or \"Untitled Article\"\n",
    "            np_text = article.text.strip()\n",
    "            if np_text:\n",
    "                print(f\"ğŸ“„ Newspaper3k: {len(np_text.split())} words\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Newspaper3k failed: {e}\")\n",
    "\n",
    "        # Try Trafilatura\n",
    "        try:\n",
    "            downloaded = trafilatura.fetch_url(url)\n",
    "            if downloaded:\n",
    "                tf_text = trafilatura.extract(downloaded) or \"\"\n",
    "                if tf_text:\n",
    "                    print(f\"ğŸ“„ Trafilatura: {len(tf_text.split())} words\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Trafilatura failed: {e}\")\n",
    "\n",
    "        # Validate Newspaper3k first\n",
    "        if np_text and is_content_relevant(np_text, title, min_words=100, keyword_match_threshold=0.2):\n",
    "            return title, np_text\n",
    "\n",
    "        # Accept Trafilatura if long enough (skip keyword match for fallback)\n",
    "        if tf_text and len(tf_text.split()) > 150:\n",
    "            return title, tf_text\n",
    "\n",
    "        print(f\"âš ï¸ Both tools returned poor content for: {url}\")\n",
    "        return title, None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Extraction error for {url}:\\n{e}\")\n",
    "        return \"Failed to fetch title\", None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b429e29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text_local(text, max_words=500):\n",
    "    chunks = text.split('. ')\n",
    "    summaries = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for sentence in chunks:\n",
    "        if len(\" \".join(current_chunk + [sentence]).split()) <= max_words:\n",
    "            current_chunk.append(sentence)\n",
    "        else:\n",
    "            chunk_text = \". \".join(current_chunk) + \".\"\n",
    "            try:\n",
    "                prompt = f\"You are and expert news Analyst. Summarize the article below in **200 words**. Be factual and detailed.:\\n\\n{chunk_text}\"\n",
    "                response = flan_t5(prompt, max_new_tokens=400, min_new_tokens= 250, do_sample=False,\n",
    "                    early_stopping=False)[0]['generated_text']\n",
    "                summaries.append(response)\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Summarization error: {e}\")\n",
    "            current_chunk = [sentence]\n",
    "\n",
    "    if current_chunk:\n",
    "        try:\n",
    "            chunk_text = \". \".join(current_chunk) + \".\"\n",
    "            prompt = f\"Summarize the following article:\\n\\n{chunk_text}\"\n",
    "            response = flan_t5(prompt, max_new_tokens=400, min_new_tokens= 250, do_sample=False,\n",
    "                    early_stopping=False)[0]['generated_text']\n",
    "            summaries.append(response)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Summarization error: {e}\")\n",
    "\n",
    "    return \" \".join(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1a27176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” [1/10] Five Keys To Helping Traditional Businesses Embrace New Tech\n",
      "âŒ Newspaper3k failed: Article `download()` failed with 403 Client Error: Max restarts limit reached for url: https://www.forbes.com/councils/forbestechcouncil/2025/06/25/five-keys-to-helping-traditional-businesses-embrace-new-tech/ on URL https://www.forbes.com/councils/forbestechcouncil/2025/06/25/five-keys-to-helping-traditional-businesses-embrace-new-tech/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (672 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Trafilatura: 995 words\n",
      "\n",
      "ğŸ” [2/10] Feds wasted millions on tech to detect fentanyl at the border, report finds\n",
      "ğŸ“„ Newspaper3k: 386 words\n",
      "ğŸ“„ Trafilatura: 420 words\n",
      "\n",
      "ğŸ” [3/10] This Experimental Tech Allows Surgeons to See Through Blood\n",
      "ğŸ“„ Newspaper3k: 436 words\n",
      "ğŸ“„ Trafilatura: 436 words\n",
      "\n",
      "ğŸ” [4/10] Texas Tech Shooting: What We Know\n",
      "âš ï¸ Both tools returned poor content for: https://www.msn.com/en-us/news/crime/texas-tech-shooting-what-we-know/ar-AA1HkjAR\n",
      "\n",
      "ğŸ” [5/10] Tech stocks power Nasdaq 100 to a record high as markets celebrate the Israel-Iran ceasefire\n",
      "âš ï¸ Both tools returned poor content for: https://www.msn.com/en-us/money/markets/tech-stocks-power-nasdaq-100-to-a-record-high-as-markets-celebrate-the-israel-iran-ceasefire/ar-AA1HkLOI\n",
      "\n",
      "ğŸ” [6/10] Inside Kelly Services Tech And AI Journey With CIO Sean Perry\n",
      "ğŸš« Blocked domain: https://www.forbes.com/sites/peterhigh/2025/06/25/inside-kelly-services-tech-and-ai-journey-with-cio-sean-perry/\n",
      "\n",
      "ğŸ” [7/10] Intel, Walmart, other tech companies reveal 500-plus Bay Area job cuts\n",
      "ğŸ“„ Newspaper3k: 499 words\n",
      "ğŸ“„ Trafilatura: 499 words\n",
      "\n",
      "ğŸ” [8/10] Looking For Sustainable Tech? This New Repairable Phone May Be For You\n",
      "ğŸ“„ Newspaper3k: 616 words\n",
      "ğŸ“„ Trafilatura: 641 words\n",
      "\n",
      "ğŸ” [9/10] Hackathon teams race to solve defense tech challenges as Europe boosts military capabilities\n",
      "âš ï¸ Both tools returned poor content for: https://www.msn.com/en-us/news/world/hackathon-teams-race-to-solve-defense-tech-challenges-as-europe-boosts-military-capabilities/ar-AA1Hnufz\n",
      "\n",
      "ğŸ” [10/10] The Palace of Versailles wants you to talk to statues through OpenAI's tech. But the statues don't seem that interested in chatting.\n",
      "âš ï¸ Both tools returned poor content for: https://www.msn.com/en-us/news/other/the-palace-of-versailles-wants-you-to-talk-to-statues-through-openais-tech-but-the-statues-dont-seem-that-interested-in-chatting/ar-AA1HnLf5\n"
     ]
    }
   ],
   "source": [
    "summary_db = {}\n",
    "\n",
    "for idx, article in enumerate(resolved_articles):\n",
    "    print(f\"\\nğŸ” [{idx+1}/{len(resolved_articles)}] {article['title']}\")\n",
    "    url = article[\"url\"]\n",
    "\n",
    "    title, full_text = get_article_text(url)\n",
    "    if not full_text:\n",
    "        summary_db[title] = {\n",
    "            \"topic\": article[\"topic\"],\n",
    "            \"url\": url,\n",
    "            \"summary\": \"(No summary â€” article text not available.)\",\n",
    "            \"full_text\": None\n",
    "        }\n",
    "        continue\n",
    "\n",
    "    summary = summarize_text_local(full_text)\n",
    "\n",
    "    summary_db[title] = {\n",
    "        \"topic\": article[\"topic\"],\n",
    "        \"url\": url,\n",
    "        \"summary\": summary,\n",
    "        \"full_text\": full_text\n",
    "    }\n",
    "\n",
    "    time.sleep(1)  # avoid rate-limiting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16b037bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Skipping weak article: MSN\n",
      "âš ï¸ Skipping weak article: Blocked Domain\n",
      "\n",
      "âœ… Filtered DB has 5 high-quality entries.\n"
     ]
    }
   ],
   "source": [
    "# Cleaned version for QnA\n",
    "filtered_summary_db = {}\n",
    "\n",
    "for title, data in summary_db.items():\n",
    "    summary = data.get(\"summary\", \"\")\n",
    "    full_text = data.get(\"full_text\", \"\")\n",
    "\n",
    "    if summary and len(summary.split()) > 30:\n",
    "        filtered_summary_db[title] = data\n",
    "    elif full_text and len(full_text.split()) > 50:\n",
    "        filtered_summary_db[title] = data\n",
    "    else:\n",
    "        print(f\"âš ï¸ Skipping weak article: {title}\")\n",
    "\n",
    "print(f\"\\nâœ… Filtered DB has {len(filtered_summary_db)} high-quality entries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c15d8a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Summary DB saved with 5 entries.\n"
     ]
    }
   ],
   "source": [
    "with open(\"summary_db.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(filtered_summary_db, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nâœ… Summary DB saved with {len(filtered_summary_db)} entries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a33e15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š Sample Titles in Summary DB:\n",
      "1. Untitled Article\n",
      "2. Feds wasted millions on tech to detect fentanyl at the border, report finds\n",
      "3. This Experimental Tech Allows Surgeons to See Through Blood\n",
      "4. Intel, Walmart, other tech companies reveal 500-plus Bay Area job cuts\n",
      "5. Looking For Sustainable Tech? This New Repairable Phone May Be For You\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ“š Sample Titles in Summary DB:\")\n",
    "for i, title in enumerate(list(filtered_summary_db.keys())):\n",
    "    print(f\"{i+1}. {title}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2bb9e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. ğŸ“° Title: Untitled Article\n",
      "ğŸ”— URL: https://www.forbes.com/councils/forbestechcouncil/2025/06/25/five-keys-to-helping-traditional-businesses-embrace-new-tech/\n",
      "ğŸ“ Word count: 746\n",
      "ğŸ“„ Summary Preview:\n",
      " Revv CEO Adi Bathla is transforming auto repair with AI-powered tools for ADAS calibration, diagnostics and shop efficiency. At the intersection between businesses and technology is this irony: The industries that need technology the most are often the most resistant to adopting it. Weâ€™ve seen this firsthand at Revv, which serves the automotive repair industry: Many shops are still operating as they did decades ago. Even with all the technological revolution in the automotive industry, many shop ...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "2. ğŸ“° Title: Feds wasted millions on tech to detect fentanyl at the border, report finds\n",
      "ğŸ”— URL: https://www.usatoday.com/story/news/politics/2025/06/25/feds-wasted-money-border-technology-fentanyl/84287735007/\n",
      "ğŸ“ Word count: 190\n",
      "ğŸ“„ Summary Preview:\n",
      " Customs and Border Protection mishandled a program intended to intercept drugs at the border, investigators say. The program was intended to intercept drugs at the U.S.-Mexico border, but was mishandled, investigators say. The program was mishandled between 2020 and 2024, the Department of Homeland Security's Office of the Inspector General said. The report is missing important context. The report is a rebuttal to a report by the Department of Homeland Security's Office of the Inspector General. ...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "3. ğŸ“° Title: This Experimental Tech Allows Surgeons to See Through Blood\n",
      "ğŸ”— URL: https://gizmodo.com/this-experimental-tech-allows-surgeons-to-see-through-blood-2000620162\n",
      "ğŸ“ Word count: 221\n",
      "ğŸ“„ Summary Preview:\n",
      " In a first, scientists have just found a way for surgeons to see through blood during a procedure, effectively making it transparent. On Tuesday, Ocutrx Technologies revealed the innovative tool, named HemoLucence. It reportedly uses AI-powered physics to digitally visualize blood as though it were translucent, which should give surgeons a clear view of the tissue beneath while operating. The technology is part of a surgical microscope system that the company plans to test in clinical trials as  ...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "4. ğŸ“° Title: Intel, Walmart, other tech companies reveal 500-plus Bay Area job cuts\n",
      "ğŸ”— URL: https://www.mercurynews.com/2025/06/24/economy-tech-jobs-layoff-intel-walmart-work-south-bay-chip-internet/\n",
      "ğŸ“ Word count: 186\n",
      "ğŸ“„ Summary Preview:\n",
      " The tech industry in the Bay Area is facing a series of layoffs, including 107 jobs at multiple Santa Clara locations. The layoffs come on the heels of other job losses in the tech sector in the region. The San Francisco-San Mateo region lost 2,400 jobs in May, the East Bay shed 400 and the South Bay lost 200. The company has been warning of its intentions to eliminate up to 20% of its workforce, potentially affecting 10,000 or more workers worldwide. The companies all described the job cuts as  ...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "5. ğŸ“° Title: Looking For Sustainable Tech? This New Repairable Phone May Be For You\n",
      "ğŸ”— URL: https://www.pcmag.com/news/looking-for-sustainable-tech-this-new-repairable-phone-may-be-for-you\n",
      "ğŸ“ Word count: 433\n",
      "ğŸ“„ Summary Preview:\n",
      " Fairphone has been offering Android phones for over 10 years, and its latest is the Fairphone 6 set to be released in August. Nearly two years since it last announced a phone, this is the companyâ€™s most high-spec device yet. Unlike most smartphone makers, Fairphone has decided to reduce the size of its next-gen handset moving from a 6.46-inch display on the Fairphone 5 to a 6.31-inch LTPO OLED screen with a 120Hz refresh rate. You May Also Like Fairphoneâ€™s mission is to make its phones as easy t ...\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Load summary database\n",
    "with open(\"summary_db.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    summary_db = json.load(f)\n",
    "\n",
    "# Preview full text of each article\n",
    "for i, (title, entry) in enumerate(summary_db.items(), 1):\n",
    "    print(f\"\\n{i}. ğŸ“° Title: {title}\")\n",
    "    print(f\"ğŸ”— URL: {entry['url']}\")\n",
    "    \n",
    "    summary = entry.get(\"summary\", \"\")\n",
    "    word_count = len(summary.split()) if summary else 0\n",
    "    print(f\"ğŸ“ Word count: {word_count}\")\n",
    "    \n",
    "    if not summary:\n",
    "        print(\"âš ï¸ Skipping â€” Summary missing.\")\n",
    "        continue\n",
    "    \n",
    "    # Show a short snippet for review\n",
    "    print(\"ğŸ“„ Summary Preview:\\n\", summary[:500], \"...\\n\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43d098b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
